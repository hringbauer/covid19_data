{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all fasta into one big .hdf5 file\n",
    "@author Harald Ringbauer, March 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current machine: compute-a-16-64.o2.rc.hms.harvard.edu\n",
      "HSM Computational partition detected.\n",
      "/n/groups/reich/hringbauer/git/covid19_data\n",
      "CPU Count: 32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "import sys as sys\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import socket\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import groupby\n",
    "from shutil import which\n",
    "import os\n",
    "import re as re\n",
    "import h5py\n",
    "\n",
    "### Pick the right path (whether on cluster or at home)\n",
    "socket_name = socket.gethostname()\n",
    "print(f\"Current machine: {socket_name}\")\n",
    "if socket_name == \"DESKTOP-5RJD9NC\":\n",
    "    path = \"/gitProjects/covid19_data\"   # The Path on Harald's machine\n",
    "if socket_name.startswith(\"compute-\"):\n",
    "    print(\"HSM Computational partition detected.\")\n",
    "    path = \"/n/groups/reich/hringbauer/git/covid19_data/\"  # The Path on Midway Cluster\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "    \n",
    "### Check whether required bins are available\n",
    "req_bins = [\"mafft\"] \n",
    "for b in req_bins:\n",
    "    s = which(b)\n",
    "    if not s:\n",
    "        print(f\"Make sure to install {b} and have in path. I cannot find it!\")\n",
    "        \n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "print(os.getcwd())\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")\n",
    "\n",
    "### Load Python scripts\n",
    "sys.path.append(\"./python3/\")\n",
    "from manipulate_fasta import fasta_iter_raw, fasta_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Key Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aligned_seq(fasta_path, align_id=\"Wuhan-Hu-1\"):\n",
    "    \"\"\"Load Aligned Sequence fasta.\n",
    "    Check against align_id and lgth.\n",
    "    Return String\"\"\"\n",
    "    if not os.path.exists(fasta_path):\n",
    "        raise RuntimeWarning(f\"Path does not exist: {fasta_path}\")\n",
    "    fiter = fasta_iter(fasta_path)\n",
    "    iid_re, _ = next(fiter)\n",
    "    \n",
    "    if len(align_id)>0:\n",
    "        if not (align_id in iid_re):\n",
    "            raise RuntimeWarning(f\"Reference ID {align_id} does not match {iid_re}\")\n",
    "    iid, seq = next(fiter)  # Get the Meat\n",
    "    return iid, seq\n",
    "\n",
    "def combine_fasta_alignments(paths, align_id=\"Wuhan-Hu-1\", output=True):\n",
    "    \"\"\"Load, and combine all the paths \"\"\"\n",
    "    n = len(paths)\n",
    "    \n",
    "    _, seq = load_aligned_seq(paths[0], align_id=align_id)\n",
    "    k = len(seq)\n",
    "    \n",
    "    seqs = np.empty((n,k), dtype=\"|S1\")  # Create place holder for all sequences\n",
    "    iids = np.empty(n, dtype=object) # Place holder for the iids\n",
    "    \n",
    "    for i, path in enumerate(paths):\n",
    "        if (i%1000==0) and output:\n",
    "            print(f\"Producing sequence: {i}\")\n",
    "        iid, seq = load_aligned_seq(path, align_id=align_id)\n",
    "        iids[i], seqs[i,:] = iid, list(seq) # Typecast into list so assignment works\n",
    "    return iids, seqs\n",
    "\n",
    "#######################################\n",
    "#######################################\n",
    "\n",
    "def post_process_counts(ct, drop=[b\"n\", b\"-\"]):\n",
    "    \"\"\"Post-process Pandas value Counts.\n",
    "    drop: What allele to drop and not to count\"\"\"\n",
    "    ct = ct.drop(labels = drop, errors=\"ignore\")\n",
    "    if len(ct)==1:\n",
    "        gt_ct = [ct.values[0], 0]\n",
    "        alleles = [ct.index[0], \"\"]\n",
    "    else:\n",
    "        gt_ct = [ct.values[0], ct.values[1]]\n",
    "        alleles = [ct.index[0], ct.index[1]]\n",
    "    return gt_ct, alleles\n",
    "\n",
    "def produce_allele_stats(seq, drop=[b\"n\", b\"-\"], loci=[],\n",
    "                         decode=True):\n",
    "    \"\"\"Produce allele counts for major/minor allele\n",
    "    loci: Which Loci to analyze\n",
    "    decode: Whether to decode S1 strings in output\"\"\"\n",
    "    counts = []\n",
    "    \n",
    "    if len(loci)==0:\n",
    "        k = np.shape(seqs)[1]\n",
    "        loci = range(0,k)\n",
    "        print(f\"Running all Loci: n={len(loci)}\")\n",
    "        \n",
    "    ref_count, alt_count = np.zeros(len(loci), dtype=\"int\"), np.zeros(len(loci), dtype=\"int\")\n",
    "    ref, alt  =  np.empty(len(loci), dtype=\"|S1\"), np.empty(len(loci), dtype=\"|S1\")\n",
    "    \n",
    "    for i,l in enumerate(loci):\n",
    "        ct = pd.value_counts(seqs[:,l])\n",
    "        gts_c, alleles = post_process_counts(ct, drop=drop)\n",
    "        ref_count[i], alt_count[i] = gts_c\n",
    "        ref[i], alt[i] = alleles\n",
    "    \n",
    "    df = pd.DataFrame({\"refcount\":ref_count, \"altcount\":alt_count, \n",
    "                  \"ref\":ref, \"alt\":alt, \"pos\":loci})\n",
    "    if decode:\n",
    "        df['ref'] = df['ref'].str.decode(\"utf-8\")\n",
    "        df['alt'] = df['alt'].str.decode(\"utf-8\")\n",
    "    return df\n",
    "\n",
    "####################################################\n",
    "####################################################\n",
    "### Manipulate Sequences and extract Info\n",
    "\n",
    "def extract_idcs(seqs, pos, der=\"g\"):\n",
    "    \"\"\"Extract idcs where seqs is allele \n",
    "    der on position pos. Return list of idcs\"\"\"\n",
    "    idcs = seqs[:,pos] == der\n",
    "    return idcs\n",
    "\n",
    "def give_idx_deletion(seqs, deletion, del_str=b'n', mis_frac=0.75):\n",
    "    \"\"\"Return iid of Deletion\"\"\"\n",
    "    seqs_del = seqs[:,deletion] # Extract the Sequences where there is a deletion\n",
    "    idx = seqs_del == del_str\n",
    "    print(f\"Average Rate of n: {np.mean(idx):.4f}\")\n",
    "    sum_n = np.sum(idx, axis=1)\n",
    "    iid_idx = sum_n > (len(deletion) * mis_frac)\n",
    "    print(f\"Found: {np.sum(iid_idx)} / {len(iid_idx)} Individuals\")\n",
    "    return  iid_idx, sum_n\n",
    "\n",
    "#######################################################\n",
    "\n",
    "def extract_var_df(df, frac_tot=0.75, maf=0.05):\n",
    "    \"\"\"Extract and return Dataframe with MAF > maf.\n",
    "    frac_tot: Minimum total individuals with coverage thre\"\"\"\n",
    "    tot_count = df1[\"altcount\"] + df1[\"refcount\"]\n",
    "    count_good =  tot_count>(frac_tot*np.max(tot_count))\n",
    "    print(f\"Minimum Count per Locus required: {frac_tot*np.max(tot_count)}\")\n",
    "    idx = (df1[\"altcount\"] / tot_count > maf) & count_good\n",
    "\n",
    "    print(f\"Found {np.sum(idx)} Variants with MAF>{maf}\")\n",
    "    df_var = df1[idx].copy()\n",
    "\n",
    "    df_var[\"totcount\"] = df_var[\"refcount\"] + df_var[\"altcount\"]\n",
    "    df_var[\"maf\"] = df_var[\"altcount\"]/df_var[\"totcount\"]\n",
    "    return df_var\n",
    "\n",
    "########################################################\n",
    "########################################################\n",
    "\n",
    "def save_h5(seqs, iids, path, output=True):\n",
    "        \"\"\"Create a new HDF5 File with Sequence Data.\n",
    "        Each sequence entry is saved as a single char.\n",
    "        seqs: Genomic Data: [k,l] array\n",
    "        iids: IIDs to save into HDF5.\n",
    "        path: Where to save the HDF5 to\"\"\"\n",
    "        k, l = np.shape(seqs)  # Nr of Individuals and Nr of Loci\n",
    "        assert(k == len(iids)) ## Sanity Chec\n",
    "        \n",
    "        max_len = np.max([len(iid) for iid in iids]) # Longest iid\n",
    "\n",
    "        if os.path.exists(path):  # Do a Deletion of existing File there\n",
    "            os.remove(path)\n",
    "            \n",
    "        dt = h5py.special_dtype(vlen=str)  # To have no problem with saving\n",
    "\n",
    "        with h5py.File(path, 'w') as f0:\n",
    "            ### Create all the Groups\n",
    "            f_gt = f0.create_dataset(\"gt\", (k,l), dtype='|S1')\n",
    "            f_samples = f0.create_dataset(\"samples\", (k,), dtype=dt)\n",
    "\n",
    "            ### Save the Data\n",
    "            f_gt[:] = seqs.astype(\"|S1\")\n",
    "            f_samples[:] = np.array(iids)   #.astype(\"S\" + max_len)\n",
    "\n",
    "        if output == True:\n",
    "            print(f\"Successfully saved {k} individuals to: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run and Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will load 165610 Sequences from ./output/single_seq_aligned_temp1.tsv\n",
      "CPU times: user 486 ms, sys: 79.3 ms, total: 565 ms\n",
      "Wall time: 814 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "aligned_path_fasta = \"./output/single_seq_aligned_temp1.tsv\"\n",
    "\n",
    "df = pd.read_csv(aligned_path_fasta, sep=\"\\t\", low_memory=False)\n",
    "df = df[df[\"include\"]==True].copy()\n",
    "print(f\"Will load {len(df)} Sequences from {aligned_path_fasta}\")\n",
    "### Manually filling in (next iteration has it automatic)\n",
    "#df[\"aligned_path\"] = \"./output/singleseq_aligned/\" + df[\"iid_clean\"] + \".fasta\"\n",
    "###df.to_csv(aligned_path_fasta, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull out all single Sequences\n",
    "~2000-3000 per min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "paths = df[\"aligned_path\"][131000:131100]\n",
    "iids, seqs = combine_fasta_alignments(paths, align_id=\"Wuhan-Hu-1\")  # Sanity checks align_id too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths[131000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create HDF5 of Covid-19 sequences, with iids!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seqs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seqs' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_h5(seqs, iids, path=\"./output/h5/covid_seqs_oct.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run and Save Allelic Frequency Spectrum (SNPs)\n",
    "Takes about 1 min for 120 seqences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running all Loci: n=29903\n",
      "CPU times: user 13min 25s, sys: 5.51 s, total: 13min 31s\n",
      "Wall time: 13min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df1 = produce_allele_stats(seqs, loci=[], drop=[b\"-\", b\"n\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 29903 Loci Statistics to ./output/tables/allele_spectrum_oct20.tsv\n"
     ]
    }
   ],
   "source": [
    "savepath = \"./output/tables/allele_spectrum_oct20.tsv\"\n",
    "df1.to_csv(savepath, sep=\"\\t\", index=False)\n",
    "print(f\"Saved {len(df1)} Loci Statistics to {savepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Variants Table \n",
    "Run analyze_sequences to create MAF Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 29903 Loci Statistics from ./output/tables/allele_spectrum_oct20.tsv\n",
      "Max Ref Count: 165560\n",
      "CPU times: user 19.9 ms, sys: 13.7 ms, total: 33.6 ms\n",
      "Wall time: 71.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "savepath = \"./output/tables/allele_spectrum_oct20.tsv\"\n",
    "df1 = pd.read_csv(savepath, sep=\"\\t\")\n",
    "print(f\"Loaded {len(df1)} Loci Statistics from {savepath}\")\n",
    "mx_ref = np.max(df1[\"refcount\"])\n",
    "print(f\"Max Ref Count: {mx_ref}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Count per Locus required: 66224.40000000001\n",
      "Found 78 Variants with MAF>0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "c>t    39\n",
       "g>t    12\n",
       "t>c     9\n",
       "g>a     6\n",
       "a>g     5\n",
       "g>c     3\n",
       "c>g     1\n",
       "a>t     1\n",
       "t>a     1\n",
       "c>a     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Showcase what variants we have\n",
    "df_var = extract_var_df(df1, maf = 0.01, frac_tot = 0.4)\n",
    "\n",
    "### Which Mutations\n",
    "mutations = df_var[\"ref\"] + \">\" + df_var[\"alt\"]\n",
    "mutations.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var.to_csv(\"./output/tables/variant_maf_oct.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(df_var[\"maf\"]>0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Individual Table (dataframe) with common SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_der_df(df, df_var, seqs):\n",
    "    \"\"\"Modify Dataframe df to contain\n",
    "    derived Variant columns from df_var, \n",
    "    according to seqs\"\"\"\n",
    "    df_der = df.copy()  # Copy from original dataframe\n",
    "    \n",
    "    for index, row in df_var.iterrows():\n",
    "        idcs = extract_idcs(seqs, pos=row[\"pos\"], der=row[\"alt\"].encode())\n",
    "        assert(np.sum(idcs) == row[\"altcount\"])  # Sanity Check\n",
    "        df_der[str(row[\"pos\"])] = idcs \n",
    "    return df_der\n",
    "#idcs = extract_idcs(seqs, pos=23402, der=b\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 952 ms, sys: 99.6 ms, total: 1.05 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_der = create_der_df(df, df_var, seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 165610 Individuals with derived Data to ./output/tables/derived_common_snps_oct20.tsv\n"
     ]
    }
   ],
   "source": [
    "### And now save it\n",
    "savepath = \"./output/tables/derived_common_snps_oct20.tsv\"\n",
    "df_der.to_csv(savepath, sep=\"\\t\", index=False)\n",
    "print(f\"Saved {len(df_der)} Individuals with derived Data to {savepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addtional Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#savepath=\"./figures/share/sprotein.png\"\n",
    "def plot_ref_alt(df1, savepath=\"\", xlim=[], leg_loc=\"center left\",\n",
    "                title=\"\"):\n",
    "    \"\"\"Plot Reference and Alt Count along the genome\"\"\"\n",
    "    plt.figure(figsize=(8,4))\n",
    "    ax=plt.gca()\n",
    "    ax.plot(df1[\"refcount\"], \"o\", label=\"Ref Allele Count\", ms=4)\n",
    "    ax.plot(df1[\"altcount\"], \"o\", label=\"Alt Allele Count\", ms=4)\n",
    "    ax.set_xlabel(\"Reference Position [Wuhan-Hu-1]\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.legend(loc=leg_loc)\n",
    "    if len(xlim)>0:\n",
    "        ax.set_xlim(xlim)\n",
    "    if len(savepath)>0:\n",
    "        plt.savefig(savepath, bbox_inches = 'tight', pad_inches = 0, dpi=300)\n",
    "        print(f\"Saved to {savepath}\")\n",
    "    if len(title)>0:\n",
    "        ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_ref_alt(df1, xlim=[21400,25400], title=\"Zoom In S Protein\",\n",
    "#             savepath = \"./figures/dumpster/zoom_in_s.png\")\n",
    "\n",
    "plot_ref_alt(df1, xlim=[20000,23600], title=\"Zoom In S Protein\",\n",
    "             savepath = \"./figures/dumpster/zoom_in_s.png\")\n",
    "#plot_ref_alt(df1, xlim=[22600,23200], title=\"Zoom In S Protein Deletion?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get IDX of deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_idx, sum_n =give_idx_deletion(seqs, deletion = np.arange(22900,23101))  # Big deletion in S\n",
    "iid_idx, sum_n =give_idx_deletion(seqs, deletion = np.arange(22400,22500))  # Small deletion in S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,8))\n",
    "ax=plt.gca()\n",
    "ax.hist(sum_n, bins=100, ec=\"k\")\n",
    "ax.set_title(\"Missing Loci per sequence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load single aligned H5 sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HDF5\n",
      "['gt', 'samples']\n",
      "(165610, 29903)\n"
     ]
    }
   ],
   "source": [
    "path = \"./output/h5/covid_seqs_oct.h5\"\n",
    "f = h5py.File(path, \"r\") # Load for Sanity Check. See below!  \n",
    "print(\"Loaded HDF5\")\n",
    "print(list(f))\n",
    "print(np.shape(f[\"gt\"]))\n",
    "#assert(np.shape(f[\"gt\"])[0]==len(f[\"samples\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "seqs = f[\"gt\"][:] # Takes about 15s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 904 µs, sys: 1 ms, total: 1.91 ms\n",
      "Wall time: 1.92 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iids1 = f[\"gt\"][10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv(\"./data/apr20/metadata.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hCoV-19/USA/WA-S88/2020|EPI_ISL_417141|2020-03-01',\n",
       "       'hCoV-19/USA/WA-S89/2020|EPI_ISL_417142|2020-02-29',\n",
       "       'hCoV-19/USA/WA-S87/2020|EPI_ISL_417140|2020-03-01', ...,\n",
       "       'hCoV-19/USA/WA-UW41/2020|EPI_ISL_415606|2020-03-08',\n",
       "       'hCoV-19/USA/WA-UW44/2020|EPI_ISL_415609|2020-03-08',\n",
       "       'hCoV-19/USA/WA-UW43/2020|EPI_ISL_415608|2020-03-08'], dtype=object)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = f[\"samples\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = df_meta[\"gisaid_epi_isl\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi2 = [s.split(\"|\")[1] for s in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3343"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(epi2+list(epi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3325"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f[\"samples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3323"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df_meta[\"strain\"].str.contains('|'.join(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3322"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strain</th>\n",
       "      <th>virus</th>\n",
       "      <th>gisaid_epi_isl</th>\n",
       "      <th>genbank_accession</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>country</th>\n",
       "      <th>division</th>\n",
       "      <th>location</th>\n",
       "      <th>country_exposure</th>\n",
       "      <th>...</th>\n",
       "      <th>length</th>\n",
       "      <th>host</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>originating_lab</th>\n",
       "      <th>submitting_lab</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date_submitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3312</th>\n",
       "      <td>bat/Yunnan/RaTG13/2013</td>\n",
       "      <td>ncov</td>\n",
       "      <td>EPI_ISL_402131</td>\n",
       "      <td>?</td>\n",
       "      <td>2013-07-24</td>\n",
       "      <td>Asia</td>\n",
       "      <td>China</td>\n",
       "      <td>Yunnan</td>\n",
       "      <td>Pu'er</td>\n",
       "      <td>China</td>\n",
       "      <td>...</td>\n",
       "      <td>29855</td>\n",
       "      <td>Rhinolophus affinis</td>\n",
       "      <td>?</td>\n",
       "      <td>Male</td>\n",
       "      <td>Wuhan Institute of Virology, Chinese Academy o...</td>\n",
       "      <td>Wuhan Institute of Virology, Chinese Academy o...</td>\n",
       "      <td>Zhu et al</td>\n",
       "      <td>https://www.gisaid.org</td>\n",
       "      <td>?</td>\n",
       "      <td>2020-01-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      strain virus  gisaid_epi_isl genbank_accession  \\\n",
       "3312  bat/Yunnan/RaTG13/2013  ncov  EPI_ISL_402131                 ?   \n",
       "\n",
       "            date region country division location country_exposure  ...  \\\n",
       "3312  2013-07-24   Asia   China   Yunnan    Pu'er            China  ...   \n",
       "\n",
       "     length                 host  age   sex  \\\n",
       "3312  29855  Rhinolophus affinis    ?  Male   \n",
       "\n",
       "                                        originating_lab  \\\n",
       "3312  Wuhan Institute of Virology, Chinese Academy o...   \n",
       "\n",
       "                                         submitting_lab    authors  \\\n",
       "3312  Wuhan Institute of Virology, Chinese Academy o...  Zhu et al   \n",
       "\n",
       "                         url title date_submitted  \n",
       "3312  https://www.gisaid.org     ?     2020-01-24  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta[~idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hCoV-19/bat/Yunnan/RaTG13/2013|EPI_ISL_402131|2013-07-24']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in samples if \"bat\" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Human                  3299\n",
       "Manis javanica            9\n",
       "human                     9\n",
       "Environment               4\n",
       "Canine                    1\n",
       "Rhinolophus affinis       1\n",
       "Name: host, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta[\"host\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.75"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "165*5 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
